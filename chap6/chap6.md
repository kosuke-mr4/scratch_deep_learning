# chap6

### 最適化

SDG

Momentum

AdaGrad

Adam

weight decay : 重みパラメータが減少する学習
→ 重みを 0 にすると全ての重みが同じ更新をされてしまって、重みをたくさん持つ理由も無くなるのでダメだよ

Xavier : 初期値に前層から n 個のノードの接続がある場合、1/√n の標準偏差をもつ分布を初期値として扱う

Batch Normalization : ミニバッチごとに正規化を行う

初期値にロバスト => 初期値にあまり依存しない

#### 過学習

- パラメータを大量に持ち表現力が高いモデル
- 訓練データが少ない

weight decey : 大きな重みにペナルティを持たせる -> 損失関数に対して重みの L2 ノルムを加算する

ニューラルネットワークの学習：損失関数の値を小さくすることが目的

DropOut：隠れ層のニューロンをランダムに選び出し、消去する

アンサンブル学習：複数のモデルの学習の出力を平均する => 擬似的に DropOut と同じことをしていると取れる

#### ハイパーパラメータ

- 各層のニューロン数
- バッチサイズ
- パラメータの更新時の学習係数
- Weight decay
  など

検証データ：ハイパーパラメータの調整用データ
