# chap6

### 最適化

SDG

Momentum

AdaGrad

Adam

weight decay : 重みパラメータが減少する学習
→ 重みを 0 にすると全ての重みが同じ更新をされてしまって、重みをたくさん持つ理由も無くなるのでダメだよ

Xavier : 初期値に前層から n 個のノードの接続がある場合、1/√n の標準偏差をもつ分布を初期値として扱う
